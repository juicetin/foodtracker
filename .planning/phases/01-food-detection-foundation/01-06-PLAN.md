---
phase: 01-food-detection-foundation
plan: 06
type: execute
wave: 4
depends_on: ["01-02", "01-05"]
files_modified:
  - apps/mobile/src/ml/models/.gitkeep
  - apps/mobile/src/ml/FoodDetector.ts
  - apps/mobile/src/ml/DishClassifier.ts
  - apps/mobile/src/ml/BinaryClassifier.ts
  - apps/mobile/src/ml/PortionEstimator.ts
  - apps/mobile/src/ml/InferenceRouter.ts
  - apps/mobile/src/ml/types.ts
  - apps/mobile/package.json
autonomous: false

must_haves:
  truths:
    - "Mobile app can load a TFLite/CoreML model and run inference on an image buffer"
    - "Three-stage pipeline runs in sequence: binary gate -> detection -> classification"
    - "Detected dish name triggers knowledge graph lookup returning ingredient list"
    - "Portion estimate is returned alongside detection results"
    - "User can point the app at a food photo and see identified food items with ingredients and portions"
  artifacts:
    - path: "apps/mobile/src/ml/InferenceRouter.ts"
      provides: "Orchestrates the three-stage detection pipeline + knowledge graph lookup + portion estimation"
    - path: "apps/mobile/src/ml/FoodDetector.ts"
      provides: "YOLO26 detection model wrapper using react-native-fast-tflite"
    - path: "apps/mobile/src/ml/DishClassifier.ts"
      provides: "YOLO26 classification model wrapper"
    - path: "apps/mobile/src/ml/BinaryClassifier.ts"
      provides: "Food/not-food binary classification wrapper"
    - path: "apps/mobile/src/ml/PortionEstimator.ts"
      provides: "TypeScript port of portion estimation with fallback chain"
    - path: "apps/mobile/src/ml/types.ts"
      provides: "Type definitions for detection results, ingredients, portions"
  key_links:
    - from: "apps/mobile/src/ml/InferenceRouter.ts"
      to: "apps/mobile/src/ml/FoodDetector.ts"
      via: "Calls detector after binary gate passes"
      pattern: "FoodDetector.*predict|detect"
    - from: "apps/mobile/src/ml/InferenceRouter.ts"
      to: "apps/mobile/src/data/food-knowledge.db"
      via: "SQLite query for ingredient lookup after dish classification, with class name normalization (hyphens->spaces) bridging YOLO output to knowledge graph"
      pattern: "normalizeDishName|knowledge.*query|sqlite.*dish"
    - from: "apps/mobile/src/ml/InferenceRouter.ts"
      to: "apps/mobile/src/ml/PortionEstimator.ts"
      via: "Estimates portion for each detected food item"
      pattern: "PortionEstimator.*estimate"
---

<objective>
Integrate the trained YOLO26 models, knowledge graph, and portion estimator into the React Native mobile app using react-native-fast-tflite, creating the end-to-end food analysis pipeline from photo input to ingredient + portion output.

Purpose: This is the integration plan that connects all Phase 1 components into a working mobile feature. After this plan, a user can select a food photo and see detected items, ingredients, and portion estimates -- the core Phase 1 success criteria.

Output: Working mobile ML inference pipeline in `apps/mobile/src/ml/` with full food analysis from photo to ingredients.
</objective>

<execution_context>
@/Users/jting/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jting/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-food-detection-foundation/01-RESEARCH.md
@.planning/phases/01-food-detection-foundation/01-CONTEXT.md
@.planning/phases/01-food-detection-foundation/01-02-SUMMARY.md
@.planning/phases/01-food-detection-foundation/01-05-SUMMARY.md
@apps/mobile/package.json
@apps/mobile/App.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Set up react-native-fast-tflite and create model wrappers</name>
  <files>
    apps/mobile/package.json
    apps/mobile/src/ml/types.ts
    apps/mobile/src/ml/BinaryClassifier.ts
    apps/mobile/src/ml/FoodDetector.ts
    apps/mobile/src/ml/DishClassifier.ts
    apps/mobile/src/ml/models/.gitkeep
  </files>
  <action>
Install ML dependencies and create model wrappers:

1. **Install react-native-fast-tflite:**
   ```bash
   cd apps/mobile && npm install react-native-fast-tflite
   # Also install expo-asset for loading bundled model files
   npm install expo-asset
   # Install expo-sqlite for knowledge graph queries on device
   npm install expo-sqlite
   ```

2. **Create `apps/mobile/src/ml/types.ts`** with shared type definitions:
   ```typescript
   export interface BoundingBox {
     x1: number; y1: number; x2: number; y2: number;  // pixel coordinates
     confidence: number;
     classId: number;
     className: string;
   }

   export interface DetectionResult {
     isFood: boolean;
     binaryConfidence: number;
     detections: FoodDetection[];
     totalTimeMs: number;
   }

   export interface FoodDetection {
     boundingBox: BoundingBox;
     dishName: string;
     dishConfidence: number;
     cuisine: string;
     ingredients: Ingredient[];
     portion: PortionEstimate;
   }

   export interface Ingredient {
     name: string;
     weightPct: number;
     typicalAmountG: number;
     usdaFdcId: number | null;
     isNutritionSignificant: boolean;
   }

   export interface PortionEstimate {
     weightG: number;
     confidence: 'high' | 'medium' | 'low';
     method: 'geometry' | 'user_history' | 'usda_default';
     suggestReference: boolean;
   }
   ```

3. **Create `apps/mobile/src/ml/BinaryClassifier.ts`:**
   - Load the food-binary TFLite model using `loadTensorflowModel()`
   - Implement `classify(imageBuffer: Uint8Array): Promise<{isFood: boolean, confidence: number}>`
   - Preprocess image: resize to 224x224, normalize pixels to [0,1]
   - Parse output: 2-class softmax, threshold at 0.5
   - Use CoreML delegate on iOS for GPU acceleration: `loadTensorflowModel(model, 'core-ml')`

4. **Create `apps/mobile/src/ml/FoodDetector.ts`:**
   - Load the food-detect TFLite model
   - Implement `detect(imageBuffer: Uint8Array, imageWidth: number, imageHeight: number): Promise<BoundingBox[]>`
   - Preprocess image: resize to 640x640, normalize
   - Parse YOLO26 NMS-free output tensor: [batch, num_detections, 6] -> [x1, y1, x2, y2, confidence, class_id]
   - Filter by confidence threshold (default 0.25)
   - Scale box coordinates back to original image dimensions
   - Use CoreML delegate on iOS

5. **Create `apps/mobile/src/ml/DishClassifier.ts`:**
   - Load the food-dish TFLite model
   - Implement `classify(croppedImageBuffer: Uint8Array): Promise<{className: string, confidence: number, top5: Array<{name: string, confidence: number}>}>`
   - Preprocess: resize crop to 224x224, normalize
   - Parse output: softmax over all dish classes, return Top-1 and Top-5
   - Load class names from a bundled JSON file (exported alongside the model)

6. **Create `apps/mobile/src/ml/models/.gitkeep`:**
   - Placeholder directory for .tflite model files (models are large binaries, will be added separately or via LFS)
   - Add a README note: "Copy .tflite files from training/models/mobile/ into this directory"

Important:
- react-native-fast-tflite uses JSI (zero-copy), so performance should be near-native
- On iOS, the CoreML delegate handles GPU acceleration automatically
- Model loading is async and should happen once at app startup, not per-inference
- Implement a `ModelManager` singleton that loads all three models once and provides them to the wrappers
  </action>
  <verify>
- `cd apps/mobile && npm ls react-native-fast-tflite` shows the package installed
- `apps/mobile/src/ml/types.ts` exports all detection/ingredient/portion types
- `apps/mobile/src/ml/BinaryClassifier.ts`, `FoodDetector.ts`, `DishClassifier.ts` each export a class with the specified interface
- TypeScript compilation: `cd apps/mobile && npx tsc --noEmit` passes without errors in the ml/ directory
  </verify>
  <done>
- react-native-fast-tflite installed with CoreML delegate support
- Three model wrapper classes created with proper preprocessing and output parsing
- Shared type definitions cover the full detection pipeline's data flow
- Model loading uses JSI zero-copy for optimal performance
  </done>
</task>

<task type="auto">
  <name>Task 2: Build inference router, portion estimator, and knowledge graph integration</name>
  <files>
    apps/mobile/src/ml/PortionEstimator.ts
    apps/mobile/src/ml/InferenceRouter.ts
  </files>
  <action>
Create the orchestration layer that connects all components:

1. **Create `apps/mobile/src/ml/PortionEstimator.ts`:**
   - TypeScript port of the Python `PortionEstimator` from `training/portion_estimator.py`
   - Implement the same three-tier fallback chain:
     - Level 1: Reference object geometry (plate/bowl size ratios)
     - Level 2: USDA standard serving size (from knowledge graph DB)
     - Level 3: User history extrapolation (from AsyncStorage or local DB)
   - Include the food density table for common categories
   - Return `PortionEstimate` type with confidence and suggestReference flag
   - For v1: Levels 1 and 2 only (user history requires tracking UI from Phase 4)

2. **Create `apps/mobile/src/ml/InferenceRouter.ts`:**
   - Main orchestrator class `InferenceRouter`:

   ```typescript
   export class InferenceRouter {
     private binaryClassifier: BinaryClassifier;
     private foodDetector: FoodDetector;
     private dishClassifier: DishClassifier;
     private portionEstimator: PortionEstimator;
     private knowledgeDb: SQLiteDatabase;

     async initialize(): Promise<void> {
       // Load all models (do this once at app startup)
       // Open knowledge graph SQLite DB
     }

     async analyzePhoto(
       imageUri: string,
       imageWidth: number,
       imageHeight: number,
     ): Promise<DetectionResult> {
       const startTime = Date.now();

       // Stage 1: Binary food/not-food gate
       const binaryResult = await this.binaryClassifier.classify(imageBuffer);
       if (!binaryResult.isFood) {
         return { isFood: false, binaryConfidence: binaryResult.confidence, detections: [], totalTimeMs: Date.now() - startTime };
       }

       // Stage 2: Object detection (bounding boxes)
       const boxes = await this.foodDetector.detect(imageBuffer, imageWidth, imageHeight);

       // Stage 3: For each detection, classify dish + look up ingredients + estimate portion
       const detections: FoodDetection[] = [];
       for (const box of boxes) {
         // Crop image to bounding box
         const crop = cropImage(imageBuffer, box, imageWidth, imageHeight);

         // Classify the crop
         const dishResult = await this.dishClassifier.classify(crop);

         // Look up ingredients in knowledge graph
         const ingredients = await this.lookupIngredients(dishResult.className);

         // Estimate portion
         const portion = this.portionEstimator.estimate(box, { width: imageWidth, height: imageHeight }, dishResult.className, []);

         detections.push({
           boundingBox: box,
           dishName: dishResult.className,
           dishConfidence: dishResult.confidence,
           cuisine: '', // from knowledge graph
           ingredients,
           portion,
         });
       }

       return {
         isFood: true,
         binaryConfidence: binaryResult.confidence,
         detections,
         totalTimeMs: Date.now() - startTime,
       };
     }

     private normalizeDishName(yoloClassName: string): string {
       // YOLO class names use hyphens (e.g., "fried-rice")
       // Knowledge graph uses spaces (e.g., "fried rice")
       // Normalize: lowercase, replace hyphens with spaces, trim
       return yoloClassName.toLowerCase().replace(/-/g, ' ').trim();
     }

     private async lookupIngredients(dishName: string): Promise<Ingredient[]> {
       // Step 1: Normalize YOLO class name to knowledge graph format
       const normalized = this.normalizeDishName(dishName);

       // Step 2: Try exact match in knowledge graph
       // Step 3: If no exact match, use FTS5 prefix search (fuzzy matching)
       // Step 4: If still no match, use get_best_guess() which always returns something
       //         (per locked decision: always return closest match, never "unrecognised")
       // Use the recursive CTE from knowledge-graph/query.py, ported to SQL string
     }
   }
   ```

   - Handle image loading: convert image URI to pixel buffer (use expo-image-manipulator or react-native-image-crop-picker for cropping)
   - Handle errors gracefully: if any stage fails, return partial results rather than crashing
   - Log timing for each pipeline stage (binary, detect, classify, lookup, portion)

Important implementation notes:
- The `cropImage` function needs to extract a rectangular region from the image buffer. Use react-native-skia or manual pixel array slicing.
- SQLite queries on mobile use `expo-sqlite` (already installed). Open the bundled `food-knowledge.db` read-only.
- The InferenceRouter should be usable from any screen via a React context or Zustand store.
- If the go/no-go decision (Plan 05) selected "hybrid", add VLM fallback logic here. If "yolo-primary", this pipeline is sufficient.
  </action>
  <verify>
- TypeScript compilation: `cd apps/mobile && npx tsc --noEmit` passes
- InferenceRouter exports `initialize()` and `analyzePhoto()` methods
- PortionEstimator implements fallback chain and returns PortionEstimate type
- InferenceRouter includes knowledge graph SQLite lookup
- Class name normalization: test that `normalizeDishName("fried-rice")` matches knowledge graph entry "fried rice" by calling lookupIngredients with 10+ YOLO-format class names and confirming each returns ingredients
  </verify>
  <done>
- InferenceRouter orchestrates full pipeline: binary -> detect -> classify -> ingredients -> portion
- PortionEstimator implements geometry + USDA default fallback chain
- Knowledge graph integration queries bundled SQLite DB for ingredient lists
- Pipeline returns complete DetectionResult with all food items, ingredients, and portions
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify end-to-end food detection on real photos</name>
  <files>apps/mobile/src/ml/InferenceRouter.ts</files>
  <action>
Present the complete food detection pipeline for user verification. The pipeline integrates:
- Three YOLO26 models (binary gate, object detection, dish classification)
- Knowledge graph with 1000+ dishes and ingredient breakdowns
- Portion estimation with smart fallback
- InferenceRouter that orchestrates the full pipeline

Verification steps for the user:
1. Start the mobile app: `cd apps/mobile && npx expo start`
2. Open the app on an iOS simulator or physical device
3. Use the photo picker to select a food photo (try: a plate of pasta, a bowl of fried rice, a hamburger)
4. Verify the app displays:
   - Whether the photo was classified as food (should be YES)
   - Bounding boxes drawn around detected food items
   - Dish name for each detected item
   - Ingredient list (e.g., for "carbonara": pasta, egg, pancetta, parmesan, black pepper)
   - Portion estimate in grams with confidence level
5. Try a non-food photo (landscape, selfie) -- should be classified as NOT food
6. Try a multi-item plate (e.g., sushi platter) -- should detect multiple items
7. Check inference time displayed -- should be less than 2 seconds on a modern phone
8. Note any misidentifications or missing ingredients for future training improvement

If models are not yet copied to the device (large binary files), test with the Python pipeline instead:
`cd training && python benchmark.py --test-image path/to/food/photo.jpg`
  </action>
  <verify>User confirms food detection works end-to-end on real photos by typing "approved" or describing issues</verify>
  <done>End-to-end food detection verified: food items identified, ingredients listed, portions estimated. Phase 1 success criteria met.</done>
</task>

</tasks>

<verification>
- All TypeScript files in apps/mobile/src/ml/ compile without errors
- InferenceRouter connects binary gate -> detection -> classification -> knowledge graph -> portion estimation
- End-to-end pipeline tested on real food photos with user verification
</verification>

<success_criteria>
- User confirms food detection pipeline works end-to-end on real photos
- Pipeline correctly identifies food items, provides ingredient lists, and estimates portions
- Non-food photos correctly rejected by binary gate
- Inference time is acceptable for interactive use (<2 seconds)
</success_criteria>

<output>
After completion, create `.planning/phases/01-food-detection-foundation/01-06-SUMMARY.md`
</output>
