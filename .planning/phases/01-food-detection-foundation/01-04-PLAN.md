---
phase: 01-food-detection-foundation
plan: 04
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - training/benchmark.py
  - training/evaluate/eval_portion.py
  - training/portion_estimator.py
  - training/evaluate/benchmark_report.md
autonomous: true

must_haves:
  truths:
    - "PaliGemma 2 3B (or equivalent on-device VLM) accuracy is measured on the same test images used for YOLO evaluation"
    - "Benchmark comparison exists: YOLO detection mAP vs VLM detection accuracy on identical test set"
    - "VLM inference latency is measured (per-image time in seconds)"
    - "Portion estimation module produces weight estimates given bounding box + dish type"
    - "Portion estimation uses the smart fallback chain: reference object geometry -> USDA standard serving -> user history"
  artifacts:
    - path: "training/benchmark.py"
      provides: "Unified benchmark: YOLO pipeline vs PaliGemma 2 3B on same test images"
    - path: "training/portion_estimator.py"
      provides: "Portion estimation from visual cues with fallback chain"
    - path: "training/evaluate/eval_portion.py"
      provides: "Portion estimation accuracy evaluation"
    - path: "training/evaluate/benchmark_report.md"
      provides: "Written comparison report for go/no-go decision"
  key_links:
    - from: "training/benchmark.py"
      to: "training/runs/detect/food-detect/weights/best.pt"
      via: "Loads YOLO model for comparison"
      pattern: "YOLO.*best\\.pt"
    - from: "training/portion_estimator.py"
      to: "knowledge-graph/query.py"
      via: "Looks up USDA standard serving size as fallback"
      pattern: "usda.*serving|standard.*serving"
---

<objective>
Benchmark PaliGemma 2 3B (quantized) against the trained YOLO pipeline on the same test images, and build the portion estimation module with smart fallback chain.

Purpose: The go/no-go decision on YOLO vs on-device VLM requires a direct accuracy comparison on identical test data. This is a locked decision requirement (DET-07). The portion estimation module (DET-03) is independent of the detection method and can be built alongside the benchmark.

Output: Benchmark comparison report (YOLO vs VLM accuracy + latency), portion estimation module with fallback chain, and portion accuracy evaluation.
</objective>

<execution_context>
@/Users/jting/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jting/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-food-detection-foundation/01-RESEARCH.md
@.planning/phases/01-food-detection-foundation/01-CONTEXT.md
@.planning/phases/01-food-detection-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Benchmark YOLO pipeline vs PaliGemma 2 3B on identical test set</name>
  <files>
    training/benchmark.py
    training/evaluate/benchmark_report.md
  </files>
  <action>
Create a unified benchmark comparing YOLO detection/classification pipeline against PaliGemma 2 3B:

1. Create `training/benchmark.py`:

   a. **Test set preparation:**
      - Select 200-500 representative test images spanning all priority cuisines
      - Use images from `training/datasets/food-detection-merged/images/test/` that have ground truth labels
      - Ensure balanced representation: ~equal images per cuisine group
      - Also include 50 non-food images for binary classification testing

   b. **YOLO pipeline benchmark:**
      - Load all three trained YOLO models (binary, detect, classify)
      - Run the full three-stage pipeline on each test image:
        1. Binary gate: is it food? (time this step)
        2. Detection: bounding boxes (time this step)
        3. Classification: dish name per crop (time this step)
      - Record per-image: total time, detection count, class predictions, confidences
      - Calculate: overall mAP@0.5, per-cuisine mAP, average latency per image

   c. **PaliGemma 2 3B benchmark:**
      - Load PaliGemma 2 3B from HuggingFace: `google/paligemma2-3b-224`
      - Apply INT8 quantization using `transformers` BitsAndBytesConfig (if GPU available) or load FP16
      - For each test image, prompt: "Detect all food items in this image. For each item, provide: bounding box coordinates, food name, and confidence score."
      - Parse VLM response into structured detections (bounding boxes + labels)
      - Record per-image: total time, detection count, parsed predictions
      - Calculate: detection accuracy (how many ground truth items correctly identified), average latency

   d. **Comparison report generation:**
      - Side-by-side comparison table:
        ```
        Metric                | YOLO Pipeline | PaliGemma 2 3B
        ---------------------|---------------|----------------
        Overall mAP@0.5      |    0.65       |    0.72
        Western mAP          |    0.72       |    0.75
        Chinese mAP          |    0.58       |    0.70
        ...
        Avg latency (ms)     |    45         |    4500
        Model size (MB)      |    12         |    3000
        Deterministic         |    Yes        |    No
        ```
      - Write report to `training/evaluate/benchmark_report.md`

   e. **Hybrid routing analysis:**
      - For images where YOLO confidence < 0.6 and VLM got it right: count these as "hybrid wins"
      - Calculate: "If we use YOLO primary + VLM fallback at conf < 0.6, what's the combined accuracy?"
      - Report the optimal confidence threshold for hybrid routing

   Important notes:
   - PaliGemma 2 3B requires significant GPU memory (~6GB FP16, ~3GB INT8). If not available on local machine, document this and run on CPU (will be slow but accuracy is what matters, not benchmark latency on dev machine).
   - If PaliGemma model download/loading fails (common for 3B models), fall back to PaliGemma 2 1B or Florence-2 and document the substitution.
   - VLM output parsing is unreliable. Build a robust parser that handles various response formats.
   - Per locked decision: "If YOLO hits 85% but LLM hits 97%, invest more in YOLO training first" -- the benchmark informs but does not automatically trigger a pivot.
  </action>
  <verify>
- `python training/benchmark.py` completes and produces `training/evaluate/benchmark_report.md`
- Report contains YOLO and VLM accuracy numbers on the same test set
- Report contains per-cuisine breakdown for both methods
- Report contains latency comparison
- Report contains hybrid routing analysis with optimal confidence threshold
  </verify>
  <done>
- YOLO pipeline accuracy measured: overall and per-cuisine mAP@0.5
- VLM accuracy measured on identical test images (or documented why VLM could not be benchmarked)
- Side-by-side comparison report generated with accuracy, latency, model size, determinism
- Hybrid routing analysis shows combined accuracy at various confidence thresholds
- Report provides all data needed for the go/no-go decision in Plan 05
  </done>
</task>

<task type="auto">
  <name>Task 2: Build portion estimation module with smart fallback chain</name>
  <files>
    training/portion_estimator.py
    training/evaluate/eval_portion.py
  </files>
  <action>
Build the portion estimation module implementing the locked decision's smart fallback chain:

1. Create `training/portion_estimator.py`:

   a. **Core estimation class `PortionEstimator`:**
      ```python
      class PortionEstimator:
          def estimate(
              self,
              bounding_box: tuple,     # (x1, y1, x2, y2) in pixels
              image_size: tuple,        # (width, height) in pixels
              dish_name: str,
              reference_objects: list,  # detected reference objects [{type, bbox}]
              user_history: list = None, # past servings [{dish, weight_g}]
              exif_data: dict = None,   # camera metadata
          ) -> PortionEstimate:
              ...
      ```

   b. **Smart fallback chain** (per locked decision):
      - **Level 1: Reference object geometry** -- If a reference object is detected (plate rim, coin, credit card, hand), calculate pixel-to-cm ratio and estimate food volume using bounding box area + assumed depth. Convert volume to weight using food density lookup.
        - Standard plate diameter: 26cm (dinner), 20cm (side)
        - Standard bowl diameter: 16cm
        - Credit card: 8.56cm x 5.4cm
        - Implement food density table for common foods (rice=0.9g/cm3, meat=1.05g/cm3, vegetables=0.6g/cm3, liquids=1.0g/cm3)
      - **Level 2: USDA standard serving** -- If NO reference object and NO user history, use USDA standard serving size for the identified dish. Query knowledge graph for `typical_amount_g` or fall back to USDA serving weight.
      - **Level 3: User history extrapolation** -- If user has logged this dish before, use average of their previous portion sizes as the estimate. Weight by recency (more recent entries count more).

   c. **Confidence scoring:**
      - Return confidence level with each estimate: HIGH (reference object), MEDIUM (user history), LOW (USDA default)
      - When confidence is LOW, set `suggest_reference = True` (triggers one-time tip in UI: "Try placing a coin next to your food for more accurate portions")

   d. **PortionEstimate return type:**
      ```python
      @dataclass
      class PortionEstimate:
          weight_g: float
          confidence: str          # "high", "medium", "low"
          method: str              # "geometry", "user_history", "usda_default"
          suggest_reference: bool  # True if confidence is low
          details: dict            # Method-specific details for debugging
      ```

2. Create `training/evaluate/eval_portion.py`:
   - Create a small evaluation dataset: 20-30 food images with known weights (manually curated or from published portion estimation datasets)
   - For each image:
     - Extract bounding boxes (use trained detection model or manual annotation)
     - Run PortionEstimator with each fallback level
     - Compare estimated weight vs actual weight
   - Report:
     - Mean absolute error (MAE) in grams
     - Mean percentage error
     - Distribution of errors (within +/-10%, +/-20%, +/-30%)
     - Breakdown by estimation method (geometry vs USDA default)
   - Target: +/-10% when reference objects present (per locked decision), +/-30% otherwise
   - Note: This evaluation will be approximate at this stage. Real-world accuracy validation requires user testing in later phases.

Important: The portion estimator at this stage works on bounding boxes extracted by the detection model. It does NOT require depth estimation (Depth Anything V2 is deferred per research recommendation to start with geometric approach). If geometric accuracy is insufficient, Depth Anything V2 can be added later as an enhancement.
  </action>
  <verify>
- `python -c "from training.portion_estimator import PortionEstimator; pe = PortionEstimator(); est = pe.estimate((100,100,400,400), (640,640), 'fried rice', []); print(est)"` returns a PortionEstimate with weight_g > 0
- `python training/evaluate/eval_portion.py` produces error statistics
- PortionEstimator falls back correctly: with reference object -> geometry, without -> USDA default
- When confidence is LOW, suggest_reference is True
  </verify>
  <done>
- PortionEstimator class implements three-tier fallback: geometry -> user history -> USDA default
- Confidence scoring assigns high/medium/low with reference object suggestion
- Food density table covers common food categories
- Evaluation script measures estimation accuracy against known weights
- Accuracy report documents MAE and percentage error distribution
  </done>
</task>

</tasks>

<verification>
- Benchmark report exists comparing YOLO vs VLM accuracy, latency, and model size
- Portion estimator handles all three fallback levels correctly
- Both VLM benchmark and portion estimation evaluation produce quantitative results
</verification>

<success_criteria>
- YOLO vs VLM accuracy comparison completed with per-cuisine breakdown
- Hybrid routing analysis shows optimal confidence threshold
- Portion estimator returns weight estimates for any food with appropriate confidence level
- All data needed for the go/no-go decision is documented in benchmark_report.md
</success_criteria>

<output>
After completion, create `.planning/phases/01-food-detection-foundation/01-04-SUMMARY.md`
</output>
