---
phase: 01-food-detection-foundation
plan: 05
type: execute
wave: 3
depends_on: ["01-03", "01-04"]
files_modified:
  - training/export_mobile.py
  - training/evaluate/validate_export.py
autonomous: false

must_haves:
  truths:
    - "CoreML (.mlpackage) exports exist for all three YOLO models and produce correct predictions"
    - "TFLite (.tflite) exports exist for all three YOLO models with valid flatbuffer headers"
    - "Export validation confirms PyTorch and CoreML/TFLite outputs match within 5% on 100+ test images"
    - "A go/no-go decision has been made on detection approach based on benchmark data"
  artifacts:
    - path: "training/export_mobile.py"
      provides: "Export all models to CoreML + TFLite with INT8 quantization"
    - path: "training/evaluate/validate_export.py"
      provides: "Validation comparing PyTorch vs exported model outputs"
    - path: "training/models/mobile/"
      provides: "Exported .mlpackage and .tflite model files"
  key_links:
    - from: "training/export_mobile.py"
      to: "training/runs/*/weights/best.pt"
      via: "Loads trained PyTorch models for export"
      pattern: "YOLO.*best\\.pt"
    - from: "training/evaluate/validate_export.py"
      to: "training/models/mobile/"
      via: "Loads exported models for comparison against PyTorch"
      pattern: "coremltools|tflite"
---

<objective>
Export all trained YOLO26 models to CoreML (iOS) and TFLite (Android) formats with INT8 quantization, validate exports against PyTorch outputs to catch silent failures, and present the go/no-go decision to the user based on benchmark data.

Purpose: CoreML/TFLite export can fail silently (documented in Ultralytics issue #22309 and research pitfalls). Validation is critical before mobile integration. The go/no-go checkpoint gates the Phase 2 architecture decision.

Output: Validated mobile model files (.mlpackage, .tflite) in `training/models/mobile/`, and a user decision on primary detection method.
</objective>

<execution_context>
@/Users/jting/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jting/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-food-detection-foundation/01-RESEARCH.md
@.planning/phases/01-food-detection-foundation/01-CONTEXT.md
@.planning/phases/01-food-detection-foundation/01-03-SUMMARY.md
@.planning/phases/01-food-detection-foundation/01-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Export YOLO models to CoreML + TFLite and validate outputs</name>
  <files>
    training/export_mobile.py
    training/evaluate/validate_export.py
  </files>
  <action>
Export and validate all three YOLO26 models for mobile deployment:

1. Create `training/export_mobile.py`:

   a. **Export each model to both formats:**
      ```python
      from ultralytics import YOLO

      models = {
          "food-binary": "training/runs/classify/food-binary/weights/best.pt",
          "food-detect": "training/runs/detect/food-detect/weights/best.pt",
          "food-dish": "training/runs/classify/food-dish/weights/best.pt",
      }

      for name, path in models.items():
          model = YOLO(path)

          # CoreML export (iOS)
          model.export(format="coreml", int8=True)
          # -> produces best.mlpackage

          # TFLite export (Android)
          model.export(format="tflite", int8=True)
          # -> produces best_int8.tflite
      ```

   b. **Organize exported files:**
      - Copy all exported models to `training/models/mobile/`:
        - `food-binary.mlpackage`, `food-binary.tflite`
        - `food-detect.mlpackage`, `food-detect.tflite`
        - `food-dish.mlpackage`, `food-dish.tflite`
      - Print file sizes for each exported model

   c. **Quick sanity check:**
      - Load each exported model and run one inference to confirm no immediate errors
      - For TFLite: verify flatbuffer magic header starts with 'TFL3' (research pitfall #4)

2. Create `training/evaluate/validate_export.py`:

   a. **Comprehensive output comparison:**
      - Select 100 diverse test images (ensure cuisine balance)
      - For each model and each test image:
        1. Run PyTorch inference -> get predictions (boxes/classes/confidences)
        2. Run CoreML inference (using coremltools) -> get predictions
        3. Compare:
           - Detection models: IoU between corresponding boxes (match by class + proximity)
           - Classification models: Top-1 class match and confidence delta
        4. Flag mismatch if:
           - Detection: IoU < 0.95 for any matched box pair
           - Detection: Different number of detections (>10% difference)
           - Classification: Different Top-1 class
           - Classification: Confidence delta > 0.1
      - Note: TFLite validation on macOS requires `tflite-runtime` package. If unavailable, validate CoreML only and defer TFLite validation to Android device testing.

   b. **Report:**
      ```
      Model          | Format  | Match Rate | Avg IoU/Conf Delta | Issues
      --------------|---------|------------|--------------------|---------
      food-binary   | CoreML  |   100%     |     0.02           | None
      food-binary   | TFLite  |    98%     |     0.05           | 2 conf diffs
      food-detect   | CoreML  |    96%     |     0.91           | 4 box mismatches
      ...
      ```
      - Save to `training/evaluate/export_validation_report.txt`

   c. **Pass/Fail gate:**
      - PASS: >95% match rate across all models and formats
      - WARN: 90-95% match rate (investigate mismatches but proceed)
      - FAIL: <90% match rate (do NOT proceed to mobile integration; investigate CoreML export issues)

   If FAIL: Document the specific failure patterns (which images, which models, what kind of mismatch). Check against known Ultralytics issues (#22309, #14668). Try re-export with different settings (e.g., without INT8 quantization).
  </action>
  <verify>
- `ls training/models/mobile/` shows 6 files (3 .mlpackage + 3 .tflite)
- `python training/evaluate/validate_export.py` produces report with >90% match rate
- TFLite files have valid headers (or validation deferred to device testing with documented reason)
- Export validation report saved to `training/evaluate/export_validation_report.txt`
  </verify>
  <done>
- All three YOLO26 models exported to CoreML (.mlpackage) and TFLite (.tflite) with INT8 quantization
- Validation confirms PyTorch outputs match exported model outputs within tolerance
- Export validation report documents match rates and any issues found
- Model files organized in training/models/mobile/ ready for mobile integration
  </done>
</task>

<task type="checkpoint:decision" gate="blocking">
  <name>Task 2: Go/No-Go decision on primary detection method</name>
  <files>
    docs/adr/adr-NNN-detection-method-decision.md
  </files>
  <action>
Present the go/no-go decision to the user based on benchmark data. This is the DET-07 decision point required by the roadmap.

Prepare a summary of all benchmark results for user review:
- Read `training/evaluate/benchmark_report.md` (YOLO vs VLM comparison)
- Read `training/evaluate/detection_cuisine_report.txt` (YOLO per-cuisine mAP)
- Read `training/evaluate/classification_cuisine_report.txt` (YOLO per-cuisine accuracy)
- Read `training/evaluate/export_validation_report.txt` (export quality)

Key locked decisions to consider:
- Target accuracy: >95% correct identification
- YOLO is preferred for speed, reproducibility, reliability
- If YOLO hits 85% but LLM hits 97%, invest more in YOLO training first
- Hybrid approach acceptable (whichever returns higher confidence per detection)
- All inference must run on-device
- No paid per-photo API calls

Options:
- **yolo-primary**: YOLO-only on-device. Fast (30-50ms), deterministic, no VLM model needed (~3GB saved), simpler pipeline. May have lower accuracy on unusual/Asian dishes.
- **hybrid**: YOLO primary + on-device VLM fallback at low confidence. Best accuracy (YOLO speed for easy cases, VLM for hard cases). Two models on device (~3GB extra), more complex, VLM 5-15s for fallback.
- **invest-yolo**: Invest more in YOLO training before deciding. Honors "don't pivot prematurely". Delays Phase 2.

After user decides, record the decision in a new ADR file.
  </action>
  <verify>User has selected one of: yolo-primary, hybrid, or invest-yolo</verify>
  <done>Go/no-go decision made and recorded. Phase 2 architecture is unblocked.</done>
</task>

</tasks>

<verification>
- Exported models validated: >90% match rate between PyTorch and CoreML/TFLite
- Go/no-go decision documented with rationale
- Decision record will be added to docs/adr/ in a subsequent step
</verification>

<success_criteria>
- Mobile model files exported and validated
- User has made the detection approach decision based on benchmark data
- Phase 2 architecture is unblocked (the decision determines on-device vs cloud pipeline)
</success_criteria>

<output>
After completion, create `.planning/phases/01-food-detection-foundation/01-05-SUMMARY.md`
</output>
